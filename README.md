# Machine Learning Platform Engineer

O projeto consiste em implementar uma arquitetura completa que consome a Punk Api no
[endpoint](https://api.punkapi.com/v2/beers/random) e ingere em um Kinesis Stream que ter√° 2 consumidores.
Com os dados processados, treinar um modelo de machine learning e integrar √† arquitetura.



Tabela de conte√∫dos
=================
<!--ts-->
   * [Sobre](#sobre)
   * [Arquitetura](#arquitetura)
   * [Pr√©-Requisitos](#pre-requisitos)
   * [Instala√ß√£o](#instalacao)
   * [Como executar o projeto](#executar-terraform)
   * [Tecnologias](#tecnologias)
   * [Execu√ß√£o do Desafio](#execucao_desafio)
   * [Autor](#autor)
   * [Licen√ßa](#licenca)
   * [Refer√™ncias](#referencias)
<!--te-->

<h4 align="center"> 
	üöß üöÄ Em constru√ß√£o...  üöß
</h4>


# <a name="sobre"><a/> Sobre

O presente projeto tem como objetivo implementar uma arquitetura completa que consome a [Punk API](https://punkapi.com/) no endpoint
https://api.punkapi.com/v2/beers/random e ingere em um Kinesis Stream que ter√° 2 consumidores. 

Para isso voc√™ ser√° necess√°rio configurar:

   1. Um CloudWatch Event que dispara a cada 5 minutos uma fun√ß√£o Lambda para alimentar o Kinesis Stream que ter√° como sa√≠da:
      * Um Firehose agregando todas as entradas para guardar em um bucket S3 com o nome de `raw`.

      * Outro Firehose com um Data Transformation que pega somente os `id`, `name`, `abv`, `ibu`, `target_fg`, `target_og`, `ebc`, `srm` e `ph` das cervejas e guarda em um outro bucket S3 com o nome de `cleaned` em formato **csv**.

   2. Crie uma tabela com os dados do bucket `cleaned`.

   3. Com base nos dados da tabela `cleaned`, treine um modelo de machine learning que classifique as cervejas em seus respectivos ibus.


# <a name="arquitetura"><a/> üè¢ Arquitetura

<h1 align="center">
  <img alt="Arquitetura" title="Arquitetura" src="./images/arquitetura_original.png" />
</h1>


# <a name="pre-requisitos"><a/> ‚òëÔ∏è Pr√©-Requisitos


#### Criar conta na AWS
   * Antes de come√ßar, voc√™ vai precisar ter uma conta na AWS, para isso acesse [AWS Console](https://aws.amazon.com/).

#### Cria√ß√£o de usu√°rio/grupo no AWS IAM

   1. Entre no console da AWS e pesquise pelo servi√ßo **IAM**;
   2. No menu √† esquerda clique em "users";
   3. Clique no bot√£o "add user";
      * Insira um nome para o usu√°rio, no meu caso foi `admin`;
      * Em `Select AWS access type` marque a primeira caixa `Programmatic access Enables an access key ID and secret access key for the AWS API, CLI, SDK, and other development tools.`;
      * Clique em `Next:Permissions`;
      * Caso n√£o tenha nenhum grupo j√° criado, clique em `create group`;
      * Na janela que abrir, insira um nome para o grupo, no meu caso foi `admin_group`;
      * Em `Filter Policies` marque a op√ß√£o `AdministratorAccess` e clique em `Create group`;
      * Clique em `Next: Tags`;
      * Em `Add tags (optional)` n√£o √© necess√°rio nenhum procedimento, apenas clique em `Next: Review`;
      * Ser√° apresentado um sum√°rio do usu√°rio e grupo que ser√£o criados, confira as informa√ß√µes e se estiverem de acordo com o desejado clique em `Create user`.


#### Cria√ß√£o de Acess key

Ap√≥s criado o usu√°rio no passo anterior, realize as seguintes etapas para criar as `acess_key`:

   1. Entre no console da AWS e pesquise pelo servi√ßo **IAM**;
   2. No menu √† esquerda clique em "users";
   3. Clique no usu√°rio que voc√™ criou;
   4. Na janela que abrir, clique em `Security credentials`;
      * Clique em `Create acess key`;
      * As chaves de acesso ser√£o geradas e dever√° clicar para salvar o arquivo, pois a secret n√£o ser√° apresentada novamente.



# <a name="instalacao"><a/> üë®‚Äçüíª Instala√ß√£o

#### Instala√ß√£o e Configura√ß√£o do AWS CLI

   1. Neste projeto, estou utilizando o sistema operacional Linux. Utilize o seguinte roteiro para a instala√ß√£o [instala√ß√£o AWS CLI](https://linuxhint.com/install_aws_cli_ubuntu/)
   2. Com o AWS CLI instalado, voc√™ dever√° configurar suas credenciais:
   ```bash
   # Execute o comando abaixo para iniciar a configura√ß√£o
   $ aws configure
   ```
    * Insira a `Acess Key` e tecle Enter;
    * Insira a `Secret Key` e tecle Enter;
    * Insira o c√≥digo da regi√£o, no meu caso √© `sa-east-1`;
    * No valor formato de sa√≠da, pode deixar `None` e tecle Enter.

#### Instala√ß√£o e Configura√ß√£o do Terraform

   1. Clique no link para baixar o Terraform de acordo com seu sistema operacional [download Terraform](https://www.terraform.io/downloads.html):
      * No meu caso, estou utilizando Linux 64-bit, ap√≥s clicar no link um arquivo ser√° baixado.
   2. Descompacte o arquivo e execute os comandos abaixo: 
      * [Roteiro de Instala√ß√£o](https://learn.hashicorp.com/tutorials/terraform/install-cli?in=terraform/aws-get-started)
   ```bash
    $ echo $PATH

   # Mova o arquivo terraform para o resultado do echo $PATH no comando anterior
   $ mv ~/Downloads/terraform /usr/local/bin/

   # Verifique a instala√ß√£o do Terraform
   $ terraform -help
   ```

# <a name="executar-terraform"><a/> üöÄ Como executar o projeto (Terraform)

Navegue at√© o diret√≥rio onde os scripts terraform est√£o para executar os passos abaixo:

```bash
# Inicialize o projeto.
$ terraform init

# Exibe um plano dos servi√ßos que ser√£o criados.
$ terraform plan

# Caso queira salvar o plano de execu√ß√£o dos servi√ßos, substitua "path" por um caminho v√°lido.
$ terraform plan -out=path 

# Realiza a cria√ß√£o dos servi√ßos nos scripts extens√£o .tf. 
# Quando o Terraform solicitar que voc√™ confirme, digite yes e pressione Enter.
$ terraform apply

# Para excluir os servi√ßos, execute terraform destroy.
$ terraform destroy
```


# <a name="tecnologias"><a/> üõ† Tecnologias

As seguintes linguagens foram usadas na constru√ß√£o do projeto:

- [Python](https://www.python.org/)
- [Terraform](https://www.terraform.io/)

#### Serverless

- [AWS Lambda](https://aws.amazon.com/en/lambda/)
- [Amazon Kinesis](https://aws.amazon.com/en/kinesis/)
- [Amazon Kinesis Data Firehose](https://aws.amazon.com/en/kinesis/data-firehose/)
- [AWS Glue](https://aws.amazon.com/en/glue/)

#### Plataforma de Machine Learning

- [Amazon SageMaker](https://aws.amazon.com/en/sagemaker/)

#### Scheduler e monitoramento de servi√ßos

- [Amazon Cloudwatch](https://aws.amazon.com/en/cloudwatch/)


#### API REST

 - [Amazon API Gateway](https://aws.amazon.com/en/api-gateway/)


# <a name="execucao_desafio"><a/> ‚ùïExecu√ß√£o do Desafio

Siga os passos abaixo para a entrega do desafio:

   1. Criar uma conta gratuita na `AWS`.

   2. Voc√™ deve utilizar `Terraform` para construir a arquitetura de uma maneira reproduz√≠vel em outras contas.

   3. Todas as fun√ß√µes `Lambdas` devem ser desenvolvidas em `Python` assim como o modelo de machine learning.

   4. O modelo de machine learning deve ser apresentado em um Jupyter Notebook, local ou remoto. O arquivo do notebook estar no reposit√≥rio do github.

   5. B√¥nus: Integre o modelo de machine learning em sua arquitetura.


## Entendimento dos dados da Punk API

√â de suma import√¢ncia passar um bom tempo analisando os dados que queremos consumir, tratar e treinar modelos. Deste modo, irei apresentar alguns pontos importantes sobre a Punk API antes do projeto implementado.

Para acessar a p√°gina principal da Punk API (Vers√£o 2) clique [aqui](https://punkapi.com/documentation/v2).

Basicamente, essa API possui dados de cervejas e podem consumir esses dados de diversas formas. Para este desafio iremos utilizar o endpoint `https://api.punkapi.com/v2/beers/random` que busca uma cerveja aleat√≥ria.

Realizei uma an√°lise para verificar quantas cervejas √∫nicas existem nessa API e encontrei o valor de 325. Portanto, independente da quantidade de vezes que busquemos uma cerveja aleat√≥ria, a quantidade de cervejas √∫nicas ser√£o de 325. Este √© um valor baixo de amostras para se obter um resultado bom em uma modelagem de machine learning.

Ao buscar uma cerveja aleat√≥ria, √© poss√≠vel observar que existem muitas features, entretanto, o que iremos utilizar para o treinamento do modelo s√£o as seguintes:

- **`abv` (Alcohol By Volume)**: indica o percentual em volume da quantidade de √°lcool em uma bebida.
- **`ibu` (International Bitterness Unit)**: uma sigla para a frase International Bitter Unit e representa uma escala, de 0 a 100, que mede o potencial de amargor conferido pelos l√∫pulos √† cerveja.
- **`target_fg` (Final Gravity)**: quantidade de subst√¢ncias (a√ß√∫cares, em geral) ferment√°veis e n√£o ferment√°veis ap√≥s a fermenta√ß√£o.
- **`target_og` (Original Gravity)**: quantidade de subst√¢ncias (a√ß√∫cares, em geral) ferment√°veis e n√£o ferment√°veis ap√≥s a fervura, antes do in√≠cio da fermenta√ß√£o.
- **`ebc` (European Brewing Convention)**: classifica como cerveja clara a cor com menos de 20 unidades EBC, e como cerveja escura a bebida com 20 ou mais unidades EBC.
- **`srm` (Standard Reference Method)**: utilizada para medir as cores da cerveja. A EBC, da Europa e a SRM dos EUA. No Brasil a escala usada √© a EBC.
- **`ph`**: em qu√≠mica, pH √© uma escala num√©rica adimensional utilizada para especificar a acidez ou basicidade de uma solu√ß√£o aquosa.


Pois bem, apresentado uma breve explica√ß√£o sobre o funcionamento da Punk API e seus dados, a seguir ser√° exposto o trabalho implementado.



## Arquitetura da implementa√ß√£o

<h1 align="center">
  <img alt="Arquitetura" title="Arquitetura" src="./images/arquitetura_final.png" />
</h1>

### **Legenda**

<h1 align="left">
  <img alt="Arquitetura" title="Arquitetura" src="./images/legenda_arquitetura_final.png" />
</h1>

### Detalhes da implementa√ß√£o

**Item 1**. A cria√ß√£o da conta gratuita e todos os pr√©-requisitos para o funcionamento deste projeto foram apresentados no t√≥pico `Pr√©-Requisitos`, clique [aqui](#pre-requisitos) para acessar.


**Item 2**. Para acessar o diret√≥rio onde est√£o os scripts `Terraform` clique [aqui](https://github.com/ldynczuki/MLPlatformEngineer/tree/main/code/terraform)


**Item 3**. Para acessar o diret√≥rio onde est√£o as fun√ß√µes `Lambdas` desenvolvidas em `Python` clique [aqui](https://github.com/ldynczuki/MLPlatformEngineer/tree/main/code/terraform):
* Para a implementa√ß√£o do desafio, as fun√ß√µes `Lambda` foram compactadas em formato .zip e possuem os seguintes nomes: `lambda_data_processing.zip`, `lambda_data_processing.zip` e `lambda_call_endpoint.zip`.


**Item 4**. Treinamento do modelo de machine learning (local):
* Esta etapa faz parte do **Fluxo de Treinamento de modelo de machine learning localmente com Jupyter Notebook** apresentado na imagem da arquitetura da implementa√ß√£o.
* Conforme apresentado o item 4., foi realizado o treinamento de um modelo de machine learning (local), o qual foi utilizado o Jupyter Notebook, onde √© poss√≠vel acess√°-lo clicando [aqui](https://github.com/ldynczuki/MLPlatformEngineer/blob/main/code/models/local-notebook/model-ml-platform.ipynb). 
* Nesta etapa, criei um objeto client do `AWS Glue` para encontrar a tabela `cleaned` e a localiza√ß√£o da fonte de dados transformados no `bucket S3`. 
* Ap√≥s encontrar a localiza√ß√£o dos dados, criei um DataFrame dos dados do `bucket S3` e iniciei a an√°lise explorat√≥rio dos dados, onde foi poss√≠vel verificar que a feature `abv` √© a que possui maior correla√ß√£o com a nossa coluna target `ibu`. Deste modo, irei realizar 2 treinamentos, o primeiro utilizando todas as features e outro treinamento apenas com a feature `abv` e irei comparar os resultados finais.
* Antes do treinamento do modelo √© essencial realizar o pr√©-processamento dos dados, onde foram eliminadas as colunas n√£o utilizadas para o treinamento, tais como `id` e `name`, apliquei tamb√©m uma convers√£o destes dados para **numpy array** com o tipo de dados float32 e, por fim, a divis√£o da base de dados em treinamento e teste, em uma propor√ß√£o de 80% e 20%, respectivamente.
* Com os dados pr√©-processados, realizei o treinamento do modelo utilizando o algoritmo de regress√£o linear m√∫ltipla e simples.
* Posteriormente o treino dos modelos, foi realizada a avalia√ß√£o dos modelos utilizando as seguintes m√©tricas: **MAE** (_Mean Absolute Error_), **MSE** (_Mean Squared Error_), **RMSE** (_Root Mean Squared Error_) e	**R2 Square**.
* Foi poss√≠vel observar no DataFrame final dos resultados no [notebook](https://github.com/ldynczuki/MLPlatformEngineer/blob/main/code/models/local-notebook/model-ml-platform.ipynb) que o modelo de regress√£o linear m√∫ltipla obteve um melhor resultado.
* √â importante salientar que, com a baixa quantidade de amostras distintas n√£o foi poss√≠vel obter um bom resultado durante o treinamento e que tamb√©m existem outros algoritmos de machine learning que podem alcan√ßar melhores resultados. Todavia, o objetivo deste treinamento foi acessar os metadados do `AWS Glue`, obter a localiza√ß√£o e os dados no `bucket S3` e realizar o treinamento.


**Item 5**. Integre o modelo de machine learning em sua arquitetura:
* Esta etapa faz parte do **Fluxo de Treinamento de modelo de machine learning com SageMaker at√© o consumo do mesmo via API** apresentado na imagem da arquitetura da implementa√ß√£o.
* Escolhi o servi√ßo `SageMaker` da Amazon para o treinamento do modelo por possuir algoritmos que cont√™m m√©todos que facilitam a implanta√ß√£o e cria√ß√£o de endpoints para o consumo do servi√ßo, o que facilita na integra√ß√£o do modelo na arquitetura do projeto.
* Desenvolvi um script `Terraform` que realiza a cria√ß√£o de uma inst√¢ncia notebook no SageMaker clonando o reposit√≥rio do meu projeto do Github. Uma vez criado a inst√¢ncia denominada `sagemaker-model` clique nela para acessar seus recursos e depois clique em **`iniciar`** para para que seja poss√≠vel acessar o Jupyter Notebook/Jupyter Lab. Uma vez dentro do Jupyter acesse o diret√≥rio `MLPlatformEngineer/code/models/sagemaker-notebook/` para encontrar o [notebook](https://github.com/ldynczuki/MLPlatformEngineer/blob/main/code/models/sagemaker-notebook/model-sagemaker.ipynb) SageMaker. Caso apare√ßa a op√ß√£o para escolher o Kernel, escolha (**conda_python3**).
* As etapas iniciais do treinamento do modelo s√£o similares ao treinamento local, onde criei o objeto client do `AWS Glue` para encontrar a tabela `cleaned` e a localiza√ß√£o da fonte de dados transformados no `bucket S3`. Ap√≥s encontrar a localiza√ß√£o dos dados, criei um DataFrame dos dados do `bucket S3` e iniciei a an√°lise explorat√≥rio dos dados, a mesma an√°lise feita no notebook local.
* Na etapa de pr√©-processamento, utilizei os mesmos tratamentos do treinamento local, a fim de comparar os resultados finais entre o modelo treinado com o `Linear Regression` (scikit-learn) e `Linear Learner` (SageMaker).
* O algoritmo escolhido para o treinamento foi o `Linear Learner` que √© comparado ao `Linear Regression` do scikit-learn. Para o treinamento, √© necess√°rio criar um container de uma imagem pr√©-existente no SageMaker do modelo `linear-learner`, o qual vai ser referenciado no momento de criar o objeto de treinamento utilizando a biblioteca `Estimator` do `SageMaker`. Neste objeto, devemos passar as configura√ß√µes da inst√¢ncia de treinamento como tamb√©m hiper√¢metros e m√©tricas de avalia√ß√£o.
* Como dito acima, o interessante dos algoritmos do SageMaker √© a facilidade da implanta√ß√£o do mesmo, ap√≥s realizar o treinamento basta executar o m√©todo `deploy()` passando os par√¢metros `initial_instance_count` e `instance_type` (que s√£o par√¢metros para configurar da quantidade e o tipo da inst√¢ncia no Amazon EC2). Ap√≥s essa execu√ß√£o √© criado um endpoint para o consumo do modelo rec√©m treinado. **Observa√ß√£o**: Antes da cria√ß√£o do endpoint do modelo atualizado, √© feita uma verifica√ß√£o se existe algum endpoint antigo e faz a exclus√£o para conter apenas o do modelo mais atual.
* Uma vez que o modelo tenha sido treinado e implantado em uma inst√¢ncia da Amazon EC2, criei uma fun√ß√£o `Lambda` que enviar√° dados para o endpoint. Nessa fun√ß√£o criei um verificador que ir√° procurar por endpoints existentes em minha conta AWS que comecem por `linear-learner`, pois o endpoint criado possui a data e hora do treinamento e, para automatizar a utiliza√ß√£o do endpoint atualizado, realizei essa procura, ao inv√©s de ir na fun√ß√£o e inserir o nome do endpoint manualmente. Para acessar o diret√≥rio dessa fun√ß√£o `Lambda` clique [aqui](https://github.com/ldynczuki/MLPlatformEngineer/tree/main/code/terraform), ela est√° compactada no formato .zip com a nomenclatura `lambda_call_endpoint.zip`.
* Com a fun√ß√£o `Lambda` criada na etapa anterior, implementei uma API REST (script `Terraform`) utilizando o servi√ßo `Amazon API Gateway` que possibilita enviar requisi√ß√µes `post` com os dados para a predi√ß√£o do modelo para a fun√ß√£o `Lambda` que por fim envia para o endpoint do modelo e retorna o `score` da infer√™ncia, informando o valor o `ibu` de acordo com os valores informados na requisi√ß√£o.
* √â poss√≠vel utilizar a API Gateway pela interface do servi√ßo `Amazon API Gateway` quando por uma ferramenta externa, por exemplo o `Postman` que utilizei nesse projeto. Segue abaixo imagens da requisi√ß√£o e o retorno do resultado da infer√™ncia (predi√ß√£o). Para utilizar o `Postman` basta copiar a **url** com o endpoint criado pela API Gateway, configurar para a requisi√ß√£o ser `post` e criar um `json` com chave `data` e os valores das features de treinamento (na mesma ordem).
* Posteriormente a cria√ß√£o da API Gateway ter sido criada, clique nela para acessar suas propriedades. Conforme pode ser visto na imagem abaixo, clique na op√ß√£o "Stage" e depois no m√©todo "POST" e ser√° apresentado a URL que poder√° ser utilizada em uma ferramenta externa, tais como o Postman.
<h1 align="center">
  <img alt="Arquitetura" title="Arquitetura" src="./images/amazon_api_gateway_url.png" />
</h1>

* Podemos testar a infer√™ncia do modelo pela interface da Amazon API Gateway, para isso clique em `Resources`, depois no m√©todo `POST`, depois clique em `Test`. Por fim, ser√° apresentada a imagem abaixo, veja que no campo `Request Body` foi inserido o json de requisi√ß√£o com os dados para a infer√™ncia do valor do `ibu`. J√° no campo `Response Body` √© apresentado o resultado do score retornado pelo modelo treinado e implantado.

<h1 align="center">
  <img alt="Arquitetura" title="Arquitetura" src="./images/amazon_api_gateway.png" />
</h1>



**INSERIR IMAGEM DA REQUISI√á√ÉO VIA API GATEWAY E VIA POSTMAN**
* No final do [notebook](https://github.com/ldynczuki/MLPlatformEngineer/blob/main/code/models/sagemaker-notebook/model-sagemaker.ipynb) SageMaker √© realizado a exclus√£o do endpoint, para evitar a cobran√ßa do servi√ßo ativo.




# <a name="autor"><a/> ü§ì Autor

Lucas Dynczuki

Entre em contato! üíö

[![Linkedin Badge](https://img.shields.io/badge/-Lucas-blue?style=flat-square&logo=Linkedin&logoColor=white&link=https://www.linkedin.com/in/lucasdynczuki/)](https://www.linkedin.com/in/lucasdynczuki/) 
[![Outlook Badge](https://img.shields.io/badge/-lucas.dynczuki@outlook.com-blue?style=flat-square&logo=Outlook&logoColor=white&link=mailto:lucas.dynczuki@outlook.com)](mailto:lucas.dynczuki@outlook.com)


# <a name="licenca"><a/>  üìù Licen√ßa

[![GitHub license](https://img.shields.io/github/license/ldynczuki/MLPlatformEngineer)](https://github.com/ldynczuki/MLPlatformEngineer/blob/main/LICENSE)


Este projeto esta sobe a licen√ßa [MIT](./LICENSE).


# <a name="referencias"><a/>  üìö Refer√™ncias

https://aws.amazon.com/en/
https://aws.amazon.com/en/cli/
https://aws.amazon.com/en/lambda/
https://aws.amazon.com/en/kinesis/data-streams/
https://aws.amazon.com/en/kinesis/data-firehose/
https://aws.amazon.com/en/glue/
https://aws.amazon.com/pt/cloudwatch/
https://aws.amazon.com/en/sagemaker/
https://www.terraform.io/downloads.html
https://learn.hashicorp.com/tutorials/terraform/install-cli?in=terraform/aws-get-started
https://learn.hashicorp.com/tutorials/terraform/aws-build?in=terraform/aws-get-started
https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/lambda_function
https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/kinesis_stream
https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/kinesis_firehose_delivery_stream
https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/glue_catalog_database
https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/glue_catalog_table
https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/cloudwatch_event_rule
https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/cloudwatch_event_target
https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/iam_policy
https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/iam_role
https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/iam_role_policy_attachment
https://linuxhint.com/install_aws_cli_ubuntu/
https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html
https://michael-timbs.medium.com/linear-regression-with-aws-sagemaker-15feefb19342
https://towardsdatascience.com/using-aws-sagemakers-linear-learner-to-solve-regression-problems-36732d802ba6
https://aws.amazon.com/pt/blogs/machine-learning/creating-a-machine-learning-powered-rest-api-with-amazon-api-gateway-mapping-templates-and-amazon-sagemaker/
https://aws.amazon.com/pt/blogs/machine-learning/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda/
https://medium.com/analytics-vidhya/invoke-an-amazon-sagemaker-endpoint-using-aws-lambda-83ff1a9f5443
https://medium.com/@gisely.alves/visualiza%C3%A7%C3%A3o-de-dados-com-seaborn-2fd0defd9adb
https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html
http://blog.cervejarialeopoldina.com.br/entenda-como-funciona-a-coloracao-das-cervejas/#:~:text=Standard%20Reference%20Method%20(SRM)&text=Esse%20%C3%A9%20um%20nome%20complicado,equivalem%20a%2010%20unidades%20EBC.
https://www.cervejaemalte.com.br/blog/como-fazer-a-correcao-da-densidade/#:~:text=FG%20%3D%20Final%20Gravity%20%3D%20Densidade%20Final,e%20a%20densidade%20da%20%C3%A1gua.